# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-aBv9Iqxf4jjsDwzkug8e6OTdqSn7CA_
"""

#!cd /content/sample_data

#!ls

from voynich import VoynichManuscript
import torch
from torch import nn
from torch.nn.utils.rnn import pack_padded_sequence
from torch.utils.data import Dataset, DataLoader
from torchtext.legacy.data import Field, TabularDataset, BucketIterator	
from sklearn.metrics import classification_report
import numpy as np

class VoynichDataset(Dataset):
	"""Custom dataset."""

	def tokenizeLine(self, lineText):
		return lineText.split('.')

	def __init__(self):
		"""
		Args:
		csv_file (string): Path to the csv file with annotations.
		root_dir (string): Directory with all the images.
		transform (callable, optional): Optional transform to be applied
		on a sample.
		"""
		self.vm = VoynichManuscript("voynich-text.txt", inline_comments=False)
		self.dataset = []
		self.vocab = set()
		self.labelSet = [""]*6
		labelList = {}
		for page in self.vm.pages:
			concatLines = []
			for line in self.vm.pages[page]:
				concatLines += self.tokenizeLine(line.text)
			if self.vm.pages[page].section not in labelList:
				self.labelSet[len(labelList)] = self.vm.pages[page].section
				labelList[self.vm.pages[page].section] = len(labelList)
			if len(concatLines) > 0:
				self.dataset.append(((concatLines), labelList[self.vm.pages[page].section], len(concatLines)))

			self.vocab = self.vocab.union(set(concatLines))
		print(self.labelSet)
		vocab2index = {}
		i = 0
		for elem in self.vocab:
			vocab2index[elem] = i
			i+=1

		for i in range(0, len(self.dataset)):
			tmp = []
			for elem in self.dataset[i][0]:
				tmp.append(vocab2index[elem])
			self.dataset[i] = (torch.FloatTensor(tmp), self.dataset[i][1], self.dataset[i][2])


	def __len__(self):
		return len(self.dataset)

	def __getitem__(self, idx):
		return self.dataset[idx]

class LSTM(nn.Module):
	def __init__(self, vocab_len, embed_len, hidden_len):
		super(LSTM, self).__init__()
		self.embedding = nn.Embedding(vocab_len, embed_len, padding_idx=0)
		self.lstm = nn.LSTM(input_size=embed_len, hidden_size=hidden_len, num_layers=1, batch_first=True, bidirectional=True)
		self.dropout = nn.Dropout(0.3)
		self.linear = nn.Linear(hidden_len, 6)
		self.hidden_len = hidden_len
	def forward(self, x, x_len):
		out = self.embedding(x)
		out = self.dropout(out)
		out = pack_padded_sequence(out, x_len, batch_first=True, enforce_sorted=False)
		a, (out, b) = self.lstm(out)
		out = self.linear(out[-1])
		return out

kfold_vals = []
def train_model(model, train_dl, epochs, lr, val_dl,lS):
  optimizer = torch.optim.Adam(model.parameters(), lr=lr)
  for i in range(epochs):
    model.train()
    sum_loss = 0.0
    total = 0
    print("STARTING EPOCH "+str(i))
    for x, y, l in train_dl:
      x = x.long()
      y = y.long()
      y_pred = model(x, l)
      optimizer.zero_grad()
      loss = nn.functional.cross_entropy(y_pred, y)
      loss.backward()
      optimizer.step()
      sum_loss += loss.item()*y.shape[0]
      total += y.shape[0]
    val_loss, val_acc = validation_metrics(model, val_dl,lS)
    if i % 1 == 0:
      print("train loss %.3f, val loss %.3f, val accuracy %.3f" % (sum_loss/total, val_loss, val_acc))
  kfold_vals.append(val_acc)

def validation_metrics(model, valid_dl,lS):
	model.eval()
	correct = 0
	total = 0
	sum_loss = 0.0
	sum_rmse = 0.0
	predList = []
	yList = []
	for x, y, l in valid_dl:
		x = x.long()
		y = y.long()
		y_hat = model(x, l)
		loss = nn.functional.cross_entropy(y_hat, y)
		pred = torch.max(y_hat, 1)[1]
		correct += (pred == y).float().sum()
		predList.append(pred)
		yList.append(y)
		total += y.shape[0]
		sum_loss += loss.item()*y.shape[0]
	#print(classification_report(yList, predList, target_names=lS))

	return sum_loss/total, correct/total


from sklearn.model_selection import train_test_split,KFold,cross_val_score
vm = VoynichManuscript("voynich-text.txt", inline_comments=False)
train = VoynichDataset()
lS = train.labelSet
k_fold = 10 
total_size = len(train)
fraction = 1/k_fold
seg = int(total_size * fraction)

for i in range(k_fold):
  print("k fold:", i+1)
  trll = 0
  trlr = i * seg
  vall = trlr
  valr = i * seg + seg
  trrl = valr
  trrr = total_size

  train_left_indices = list(range(trll,trlr))
  train_right_indices = list(range(trrl,trrr))
  train_indices = train_left_indices + train_right_indices
  val_indices = list(range(vall,valr))
  print("train indices: [%d,%d),[%d,%d), test indices: [%d,%d)" %(trll,trlr,trrl,trrr,vall,valr))
  #print(train_indices)
  #print(val_indices)
        
  trainD = torch.utils.data.dataset.Subset(train,train_indices)
  valD = torch.utils.data.dataset.Subset(train,val_indices)
  dataloader = DataLoader(trainD, batch_size=1, shuffle=True, num_workers=1, drop_last=False)
  val_dl1 = DataLoader(valD, batch_size=1, shuffle=True, num_workers=1, drop_last=False)
  print(len(dataloader))
  print(len(val_dl1))
  l = LSTM(len(train.vocab), 300, 20)
  train_model(l, dataloader, 10, .01, val_dl1, lS)
print(kfold_vals)

print(sum(kfold_vals)/len(kfold_vals))
